{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a7c5c-5ee2-4210-aec8-521c672a54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "import plotly.express as px \n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import plotly.express as px \n",
    "import pickle\n",
    "import matplotlib.animation as animation\n",
    "import kaleido \n",
    "import imageio\n",
    "sys.path.insert(1, \"../src/utils/\")\n",
    "from agent import Environment\n",
    "from Q_learning import QLearningTrainer\n",
    "sys.path.insert(1, \"plotly_graph/\")\n",
    "from functions4tuto import plot_RL_2agents, plot_reward_rocket_2agents\n",
    "JOSN_file = \"params/rocket_tuto_3.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540f1bd",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Goal</span>\n",
    "\n",
    "The purpose of this tutorial is to land a rocket at a specific coordinate by adjusting its angle and booster power.\n",
    "To control these variables effectively, we will use reinforcement learning to train the rocket to learn an optimal landing strategy through trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d7405-5fcf-45ee-8abf-c1af457b58b9",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Intialize environment</span>\n",
    "\n",
    "Create an environment object with the rules defined previously. The idea is to check the new field can raise a flag to stop simulation when the criteria are reached:\n",
    "\n",
    "* 130 <= pos_x <= 150\n",
    "* 0 <= pos_y <= 10\n",
    "* -2 <= acceleration_y <= 2\n",
    "* -10 <= speed_x >= 10 \n",
    "* -10 <= speed_y >= 10 \n",
    "\n",
    "We will also control the speed of the rocket and its acceleration to keep as possible those parameters between this range:\n",
    "\n",
    "* -15 <= speed_x <= 15\n",
    "* -15 <= speed_y <= 15\n",
    "* -10 <= acceleration_x <= 10\n",
    "* -10 <= acceleration_y <= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a104d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(JOSN_file, check_model = True)\n",
    "RL = QLearningTrainer(env, num_episodes = 2000, convergence_criterion = 0.5, \n",
    "                      decay_type = \"exponential\", decrease_prob_exp = 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c5f055",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Rewards</span>\n",
    "\n",
    "Before to use reinforcement learning. , let's first understand how the agent's reward and its individual components evolve. As a reminder. \n",
    "\n",
    "* The reward formula for the booster is:\n",
    "\n",
    "$$  -2 * distance_{y\\_reward} + speed_{y\\_reward} + 0.5 * ratio_{fuel} $$\n",
    "\n",
    "* The reward formula for the alpha (rocket's angle) is:\n",
    "\n",
    "$$  -2 * distance_{x\\_reward} + speed_{x\\_reward} - 0.2* sin(angle) + 0.5 * ratio_{fuel} $$ \n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "* Distance reward: the ratio between the current distance to the target and the initial distance at time $t=0$. It reflects how far the rocket is from the landing target.\n",
    "* speed reward: a penalty applied when the absolute speed (on a given axis) exceeds a threshold. The further the speed is from the desired range, the larger the penalty.\n",
    "* angle: Penalty for the rocket's angular deviation from vertical.\n",
    "* $ratio\\_fuel$: normalized fuel quantity ($\\frac{m\\_fuel}{m\\_fuel_{t=0}}$)\n",
    "\n",
    "\n",
    "Fuel consumption depends on both booster and alpha agents:\n",
    "\n",
    "* When the booster is activated: If the booster level is 1 or 2, fuel decreases by 5 or 10 units, respectively.\n",
    "\n",
    "* When the rocket's alpha (lateral boosters that control rocket's angle) is not zero: An additional 5 units of fuel are consumed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83640060",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Reinforcemet learning</span>\n",
    "\n",
    "\n",
    "After load our environment, next step is to create QLearningTrainer objet. It will apply Qlearning algorithm. For each states, the algorithm will apply a score base to the next iteration.\n",
    "\n",
    "The Bellman equation is the value function use in reinforcement learning. \n",
    "\n",
    "$V(s,a) = V(s, a)+ \\alpha * (R+\\gamma * max(V(s', a')) - V(s, a))$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $V(s, a)$ is the estimated value of state $s$ and actions $a$\n",
    "* $s$ is the curent state\n",
    "* $s'$ is the next state\n",
    "* $a$ is action taken\n",
    "* $a'$ next possible actions that maximize reward\n",
    "* $R$ is the immediate reward received after transitioning\n",
    "* $\\gamma$ is the discount factor, which determines the importance of future rewards.\n",
    "* $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d31355",
   "metadata": {},
   "source": [
    "Different parameters are avaible. For the tutorial, you use default parameters. Before to lunch training. Lets discus about the input parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7edb26",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Learning rate</span>\n",
    "\n",
    "\n",
    "The learning rate $\\alpha$ in the Bellman equation controls the weight given to the new estimate compared to the existing estimate of the state value. A smaller learning rate means that the new estimate has less influence, and the agent is more conservative in updating its value function. A larger learning rate allows the agent to adjust its estimates more rapidly based on new information\n",
    "\n",
    "### <span style=\"color:orange\">Discount factor (gamma)</span>\n",
    "\n",
    "The discount factor $\\gamma$, is a key hyperparameter that controls how much the agent values future rewards compared to immediate rewards. For rocket landing, $\\gamma$ equal to 0.99, since we are optimizing over multiple steps to reach a safe landing.\n",
    "\n",
    "### <span style=\"color:orange\">Epsilon parameter</span>\n",
    "\n",
    "Exploration-exploitation is a fundamental trade-off in reinforcement learning, where the agent needs to balance between exploring new actions and exploiting the knowledge it has gained so far.\n",
    "\n",
    "The epsilon-greedy policy is a simple strategy that the agent uses to decide whether to explore a new action (random exploration) or exploit the current best-known action. It helps prevent the agent from getting stuck in suboptimal policies by occasionally trying new actions. The value of epsilon determines the probabilty the agent chooses a random action. A higher epsilon encourages more exploration, while a lower epsilon emphasizes exploitation of the current best-known actions.ent knowledge.\n",
    "\n",
    "The epsilon parameter is decayed over time during training. This means that, as the agent gains more experience, it tends to rely more on exploitation and less on exploration. The idea is that, as the agent learns and becomes more confident in its estimates, it gradually reduces the rate of exploration. \n",
    "\n",
    "Epsilon initailization is made with the argument **exploration_prob**. It is a list with the lowest and the highest probability values. The probability will decrease with a rate given by the argument **decrease_prob_exp**. By default epsilon is modeled with a linear decay but you can change it to a exponential decay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ece49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(RL.get_epsilon(decay_type = 'linear')[0:800], label = \"linear decay\")\n",
    "plt.plot(RL.get_epsilon(decay_type = 'exponential')[0:800], label = \"exponential decay\")\n",
    "plt.title(\"Probability to choose a random action\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"# episode\")\n",
    "plt.ylabel(\"Epsilon value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dec7d9",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">num_episodes, run_limit and convergence_criterion</span>\n",
    "\n",
    "The last three options are hyperparameters that impact computation time:\n",
    "* ``num_episodes``: The maximum number of training episodes.\n",
    "* ``run_limit``: The maximum number of iterations allowed within a single episode before it is forcefully stopped.\n",
    "* ``convergence_criterion``: The threshold used to determine convergence, based on the difference in Q-values between two iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2498998b",
   "metadata": {},
   "source": [
    "Before launching the training, it's important to understand that the Q-learning algorithm operates in a discretized space.\n",
    "The spacing between bins can significantly impact convergence. If a discretized bin groups together too many different underlying states, the Q-table may become unstable, making it difficult for the algorithm to converge.\n",
    "\n",
    "Let's examine how our space is discretized using ``RL.env.json[\"limit\"]``.\n",
    "The agents variable (booster and alpha) have 3 bins, while the state variables \"pos_x\", \"pos_y\", \"angle\", \"speed_x\", \"speed_y\", \"weight_rocket\" have 61, 81, 3, 13, 13, and 62 bins, respectively.\n",
    "\n",
    "This results in a Q-table with:\n",
    "Our Q table is composed of $$ 3 * 3 * 61 * 81 * 3 * 13 * 13 * 62 = 1.4 *10^9\\  states $$\n",
    "\n",
    "Does that mean our system will visit every possible state?\n",
    "Of course not. Remember, the Q-learning algorithm starts with a random exploration (a random walk), but over time, the agent’s exploration becomes guided by what it has already experienced.\n",
    "As a result, some states may never be visited if the agent never finds a path leading to them or if they are not useful for improving the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2edae",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Training</span>\n",
    "\n",
    "You can also load a pre-trained RL object (trained ouput are stored in ``saved_model/output_train_2agents.txt``). If you want to generate the model from scratch, start training with the command: ``RL.q_learning()``. Before to lunch it, note it will take around 40-50 min to execute 2000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f78df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model/RL_2agents.pkl', 'rb') as f:\n",
    "    RL = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL.q_learning()\n",
    "# #save model\n",
    "# with open('saved_model/RL_2agents.pkl', 'wb') as f:  # open a text file\n",
    "#     pickle.dump(RL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf49332",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL.plot_convergence(len(RL.loss_episodes) -100)\n",
    "RL.plot_convergence(len(RL.loss_episodes) -10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b147f16",
   "metadata": {},
   "source": [
    "If you loaded the pre-trained model, you'll see that training stopped at episode 677. For the last five iterations the difference between $Qtable_N - Qtable_{N-1}$ remained bellow the convergence threshold (convergence_criterion = 0.5), indicating that the learning process had stabilized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e98947",
   "metadata": {},
   "source": [
    "Let’s find out how many states we actually explored.\n",
    "To do this, we replace Q-table values equal to 0 with np.nan.\n",
    "(Except for goal-reaching states in our setup, a value of 0 typically means the state was never explored.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f59b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_explored = np.sum(RL.q_table.copy().replace(0, np.nan).count())\n",
    "\n",
    "print(\"Number of states explored {0}.\\\n",
    " Only {1:.5f}% states have been visited\".format(\n",
    "    states_explored,\n",
    "    states_explored/1397838546 *100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6283ed02",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Optimal trajectory</span>\n",
    "\n",
    "Let’s see how the rocket evolves after training.\n",
    "We start by copying the Q-table using the ``q_table`` attribute, and we replace all 0 values with ``np.nan`` to better visualize learned values.\n",
    "\n",
    "Next, we load a new environment and use its current discretized state as the starting point.\n",
    "At each iteration, we follow the action suggested by the Q-table based on the environment’s current discretized state, which is updated after every step. At the end, we save all the states visited into a data frame (dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f5c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load policy table\n",
    "policy = RL.q_table.copy().replace(0, np.nan)\n",
    "# load environnment\n",
    "env = Environment(JOSN_file, check_model = False)\n",
    "state = env.state_for_q_table()\n",
    "flag_continue = True\n",
    "while flag_continue:\n",
    "    # control while loop\n",
    "    if not policy.index.isin([str(state)]).any():\n",
    "        flag_continue = False\n",
    "        print(\"stop: no moore state\")\n",
    "        env.delete_last_states()\n",
    "        continue\n",
    "    elif env.pos_y[-1] >= 0 and env.pos_y[-1] <= 10 and env.pos_x[-1] >= 110 and env.pos_x[-1] <= 125:\n",
    "        print(\"stop: reach goal\")\n",
    "        # env.delete_last_states()\n",
    "        flag_continue = False\n",
    "        continue\n",
    "    else:\n",
    "        action = RL.call_choose_action(state, 0)\n",
    "        _, rewards, done, problem, info = env.step(action)\n",
    "        state = env.state_for_q_table()\n",
    "\n",
    "dt = pd.DataFrame(env.all_states())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3956af",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Rocket Trajectory</span>\n",
    "\n",
    "As we can see, the rocket reaches the ground slowly. Its trajectory appears more linear, and if we look at the speed profile, the reinforcement learning model keeps the speed and acceleration within the desired limit of [-10, 10]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b9a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_RL_2agents(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_rocket_2agents(env)\n",
    "\n",
    "plt.plot(dt[[\"time\"]], dt[[\"speed_x\"]], \"+-\", label = \"RL model speed_x\")\n",
    "plt.plot(dt[[\"time\"]], dt[[\"speed_y\"]], \"+-\", label = \"RL model speed_y\")\n",
    "plt.plot(dt[[\"time\"]], np.array([-10]*len(dt[[\"time\"]])), color =\"red\", label = \"Speed limit\" )\n",
    "plt.plot(dt[[\"time\"]], np.array([10]*len(dt[[\"time\"]])), color =\"red\")\n",
    "plt.title(\"Rocket speed\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Speed\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(dt[[\"time\"]], dt[[\"acceleration_x\"]], \"+-\", label = \"RL model acceleration_x\")\n",
    "plt.plot(dt[[\"time\"]], dt[[\"acceleration_y\"]], \"+-\", label = \"RL model acceleration_y\")\n",
    "plt.plot(dt[[\"time\"]], np.array([-10]*len(dt[[\"time\"]])), color =\"red\", label = \"Acceleration limit\" )\n",
    "plt.plot(dt[[\"time\"]], np.array([10]*len(dt[[\"time\"]])), color =\"red\")\n",
    "plt.title(\"Rocket acceleration\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Speed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6aea24",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Conclusion</span>\n",
    "\n",
    "With our designed reward function and discretization settings, we provide a model capable of landing the rocket at the correct coordinates, with an appropriate angle and safe speed.\n",
    "However, one of the main limitations of Q-learning is that it operates in a discrete state space, which can limit precision and scalability.\n",
    "To overcome this limitation, we can replace the Q-table with a deep neural network, allowing us to approximate the Q-function in a continuous state space—a method known as Deep Q-Learning (DQN)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
