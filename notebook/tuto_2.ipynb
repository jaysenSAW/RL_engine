{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a7c5c-5ee2-4210-aec8-521c672a54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "import plotly.express as px \n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import plotly.express as px \n",
    "sys.path.insert(1, \"../src/utils/\")\n",
    "from agent import Environment\n",
    "from Q_learning import QLearningTrainer\n",
    "sys.path.insert(1, \"plotly_graph/\")\n",
    "from functions4tuto import plotly_trajectory, control_fall_simulation, booster_reward, plotly_all_reward\n",
    "JOSN_file = \"rocket_tuto_2.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ffc6d0",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Input file</span>\n",
    "\n",
    "In the previous tutorial we defined our environment and designed reward function. The system is a rocket that falling from a starting point and the goal is to reach a target with a minimum speed and maximize fuel.\n",
    "Variable's names, their initial values and boundaries limit are defined into a JSON file (or can be given directly into a dictionary)\n",
    "\n",
    "## <span style=\"color:orange\">Variables</span>\n",
    "\n",
    "Variables can be categorize into 3 classes:\n",
    "* `states_variables` : variable used as coordinate to describe our system\n",
    "* `agent_variables` : variable use as agent. Their values are changed for each iteration\n",
    "* 3th category are other variables. They are not used to describe our environment but they can be usefull to monitor information or to compute intermediate value. \n",
    "  There is no key for this kind of variable. Consider them as variables present into `initial_values` field and that are not `states_variables` and `agent_variables`\n",
    "\n",
    "You can access to the name of state and agent variables, with the attibute `states_variables` and `agent_variables`.\n",
    "\n",
    "    \"states_variables\" : [\"pos_y\", \"acceleration_y\", \"speed_y\"],\n",
    "    \"agent_variables\" : [\"booster\"]\n",
    "\n",
    "## <span style=\"color:orange\">Initial system</span>\n",
    "\n",
    "After to name `states_variables` and `agent_variables`, next step is to define initial state.\n",
    "It will be use as environment coordinates at the beginning of each episode. Initially, it comprises \n",
    "the values of state_variables, followed by agent_variables, and finally other variables that are \n",
    "not used for the system's coordinates.\n",
    "\n",
    "    \"initial_values\" : {\n",
    "      \"pos_y\" : [175.0],\n",
    "      \"acceleration_y\": [0.0],\n",
    "      \"speed_y\": [0.0],\n",
    "      \"angle\" : [0.0],\n",
    "      \"booster\" : [0.0],\n",
    "      \"alpha\" : [0.0],\n",
    "      \"futur_pos_y\" : [175.0],\n",
    "      \"m_fuel\" : [100],\n",
    "      \"weight_rocket\" : [105],\n",
    "      \"weight_dry_rocket\" : [5],\n",
    "      \"G\" : [1.62],\n",
    "      \"m_fuel_ini\" : [100.0],\n",
    "      \"pos_y_star\": [0.0],\n",
    "      }\n",
    "\n",
    "\n",
    "## <span style=\"color:orange\">Limit</span>\n",
    "\n",
    "Q-learning algorithms model events as a Markov process. Therefore, it is necessary to discretize our environment space. \n",
    "We define lower and upper bounds, as well as the number of divisions we want to use to discretize the variable space.\n",
    "\n",
    "\n",
    "    \"limit\" : {\n",
    "      \"pos_y\" : [0.0, 300.0, 61],\n",
    "      \"acceleration_y\": [-20.0, 20.0, 21],\n",
    "      \"speed_y\": [-50.0, 50.0, 21],\n",
    "      \"angle\" : [-0.8, 0.8, 17],\n",
    "      \"booster\" : [0.0, 1.0, 3],\n",
    "      \"alpha\" : [-0.1, 0.1, 3],\n",
    "      \"m_fuel\" : [0.0, 100, 201]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4b678",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Agents's actions</span>\n",
    "    \"n_action\" : {\n",
    "      \"booster\": {\"0\" : 0.0, \"1\" : 0.5, \"2\" : 1.0}\n",
    "    }\n",
    "\n",
    "After defining the variables and their initial values, we proceed to define actions that apply to the agent variables. In this example, we have 1 agent that can take 3 actions:\n",
    "\n",
    "For the booster:\n",
    "  * \"0\": Booster is off.\n",
    "  * \"1\": Booster is turned on to half of its power.\n",
    "  * \"2\": Booster is turned on to its full power.\n",
    "\n",
    "## <span style=\"color:orange\">Actions to take</span>\n",
    "\n",
    "    \"action_to_take\" : {\n",
    "      \"booster\": {\"$booster$\" : \"$action}\n",
    "    }\n",
    "\n",
    "Actions change the agent variables by modifying their values based on the action taken, which are retrieved from the n_action dictionary. You can change default delimiter during the initilisation:\n",
    "\n",
    "`agent = Environment(json_file, delimiter = \"Char_you_want\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba311d",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">System's evolution and reward</span>\n",
    "\n",
    "Last fields are how variables evolve after agents's action and how reward are computed.\n",
    "Only variables present in field inital value are stored. Other variables present are just temporary and are lost after each iteration.\n",
    "Reward values are stored into a dictionnary, the keys are agent variable's name. \n",
    "\n",
    "### <span style=\"color:orange\">Equation variables</span>\n",
    "\n",
    "    \"equations_variables\": {\n",
    "        \"$F$\" : \"600\",\n",
    "        \"$m_fuel$\" : \"$m_fuel$ - $booster$ *10 -$angle$ *10\",\n",
    "        \"$weight_rocket$\" : \"$weight_dry_rocket$ + $m_fuel$\",\n",
    "        \"dt\" : \"0.5\",\n",
    "        \"$theta$\" : \"0.0\",\n",
    "        \"$y_0$\" : \"$pos_y$\",\n",
    "        \"$Vy_0$\" : \"$speed_y$\",\n",
    "        \"$angle$\" : \"$theta$ + $alpha$\",\n",
    "        \"$acceleration_y$\" : \"($F$/(5+$weight_rocket$) * np.cos($angle$)) * $booster$ - $G$\",\n",
    "        \"$speed_y$\" : \"($F$/(5+$weight_rocket$) * np.cos($angle$)) * $booster$ * $dt$ - $G$ * $dt$ + $Vy_0$\",\n",
    "        \"$pos_y$\": \"(0.5 * $F$/(5+$weight_rocket$) * np.cos($angle$)) * $booster$ * $dt$**2 - $G$ * $dt$**2 + $Vy_0$ * $dt$ + $y_0$\",\n",
    "        \"$futur_pos_y$\" : \"$pos_y$ + 3 * $speed_y$\"\n",
    "    },\n",
    "\n",
    "### <span style=\"color:orange\">Reward</span>\n",
    "\n",
    "The reward indicates the immediate benefit or cost associated with the action.\n",
    "The scalar feedback signal that the environment sends to the agent after it takes an action are defined into this dictionnary.\n",
    "\n",
    "    \"equations_rewards\": {\n",
    "      \"$booster$\" : \"-($pos_y$ - $pos_y_star$)**2 + $m_fuel$/$m_fuel_ini$\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Stop episode</span>\n",
    "\n",
    "stop episode when goal is reach. If feature has 1 value, its feature's value must be equal.\n",
    "In other hand, if feature has 2 values ([min_limit, max_limit]), criterion is bounded feature >= min_limit and feature <= max_limit\n",
    "\n",
    "    \"stop_episode\" : {\n",
    "      \"pos_y\" : [0, 5],\n",
    "      \"acceleration_y\" : [-2,2],\n",
    "      \"speed_y\" : [-2,2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d7405-5fcf-45ee-8abf-c1af457b58b9",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Intialize environment</span>\n",
    "\n",
    "Create an environment object with the rules defined previously. The idea is to check the new field can raise a flag to stop simulation when the criteria are reached:\n",
    "\n",
    "* 0 <= pos_y >= 10\n",
    "* -2 <= acceleration_y >= 2\n",
    "* -10 <= speed_y >= 10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc7d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment object with the rules defined previously\n",
    "env = Environment(JOSN_file, check_model = False)\n",
    "flag = \"0\"\n",
    "flag_to_continue = True\n",
    "acceleration_y_constraint = 5\n",
    "speed_y_limit = 10\n",
    "y_lower_limit = 0\n",
    "# monitor action takes for each iteration\n",
    "actions = {\"action_booster\" : []} \n",
    "while flag_to_continue:    \n",
    "    current_state, rewards, done, problem, info = env.step([flag, 1])\n",
    "    actions[\"action_booster\"].append(flag)\n",
    "    if env.futur_pos_y[-1] < 0 and env.m_fuel[-1] > 0:\n",
    "        flag = \"1\"\n",
    "    # stop engine if there is no fuel\n",
    "    elif env.m_fuel[-1] <= 0:\n",
    "        flag = \"0\"\n",
    "    elif np.abs(env.speed_y[-1]) > speed_y_limit:\n",
    "        # print(\"speed limit\")\n",
    "        if env.speed_y[-1] > 0:\n",
    "            flag = \"0\"\n",
    "        else:\n",
    "            flag = \"1\"\n",
    "    elif np.abs(env.acceleration_y[-1]) > acceleration_y_constraint:\n",
    "        # print(\"acceleration limit\")\n",
    "        if env.acceleration_y[-1] > 0:\n",
    "            flag = \"0\"\n",
    "        else:\n",
    "            flag = \"1\" \n",
    "    # stop simulation\n",
    "    if env.pos_y[-1] < 0:\n",
    "        flag_to_continue = False\n",
    "        # delete last state because rocket is bellow to the ground\n",
    "        env.delete_last_states()\n",
    "        continue\n",
    "    if any(done) and info[0] == \"Reach goal\":\n",
    "        print({val : current_state[val] for val in ['pos_y', 'acceleration_y', 'speed_y', 'booster']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "end  = 70\n",
    "dt = pd.DataFrame(env.all_states())\n",
    "plt.plot(pd.DataFrame(env.rewards)[:end], \n",
    "         label = \"reward\")\n",
    "plt.plot(np.array(\n",
    "    [-2 + np.exp(np.min([val1, 0])) + np.exp(np.min([val2, 0])) \n",
    "     for val1, val2 in zip(\n",
    "         dt[\"futur_pos_y\"] - 0, - dt[\"futur_pos_y\"] + 200\n",
    "         ) ])[:end], \"+\", label = \"boundaries penalty\")\n",
    "plt.plot(-(dt['pos_y'][:70]-dt['pos_y_star'][:end])/(dt['pos_y'].max() - dt['pos_y_star']), \n",
    "         \"+\", label = \"normalized dist\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.DataFrame(env.all_states())\n",
    "dt[\"iter\"] = np.arange(0, dt.shape[0])\n",
    "dt[\"reward\"] = env.rewards['booster']\n",
    "\n",
    "fig = px.scatter_3d(dt[:70], \n",
    "                    x='pos_y', \n",
    "                    y='speed_y', \n",
    "                    z='acceleration_y',\n",
    "              color='reward')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc9cc0",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Reinforcemet learning</span>\n",
    "\n",
    "\n",
    "After load our environment, next step is to create QLearningTrainer objet. It will apply Qlearning algorithm. For each states, the algorithm will apply a score base to the next iteration.\n",
    "\n",
    "The Bellman equation is the value function use in reinforcement learning. \n",
    "\n",
    "$v(s) = (1−\\alpha) * V(s)+ \\alpha * (R+\\gamma * V(s'))$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $V(s)$ is the estimated value of state $s$ * $s$ is the curent state\n",
    "* $s'$ is the next state.\n",
    "* $R$ is the immediate reward received after transitioning from state $s$ to state\n",
    "* $\\gamma$ is the discount factor, which determines the importance of future rewards.\n",
    "* $\\alpha$ is the learning rate.\n",
    "s.l difference (TD) learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c07191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RL object\n",
    "env = Environment(JOSN_file, check_model = True)\n",
    "RL = QLearningTrainer(env, num_episodes = 800, convergence_criterion = 0.5, decay_type = \"exponential\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d31355",
   "metadata": {},
   "source": [
    "Different parameters are avaible. For the tutorial, you use default parameters. Before to lunch training. Lets discus about the input parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c9872",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Learning rate</span>\n",
    "\n",
    "\n",
    "The learning rate $\\alpha$ in the Bellman equation controls the weight given to the new estimate compared to the existing estimate of the state value. A smaller learning rate means that the new estimate has less influence, and the agent is more conservative in updating its value function. A larger learning rate allows the agent to adjust its estimates more rapidly based on new information\n",
    "\n",
    "### <span style=\"color:orange\">Discount factor ($\\gamma$)</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:orange\">Epsilon parameter</span>\n",
    "\n",
    "Exploration-exploitation is a fundamental trade-off in reinforcement learning, where the agent needs to balance between exploring new actions and exploiting the knowledge it has gained so far.\n",
    "\n",
    "The epsilon-greedy policy is a simple strategy that the agent uses to decide whether to explore a new action (random exploration) or exploit the current best-known action. It helps prevent the agent from getting stuck in suboptimal policies by occasionally trying new actions. The value of epsilon determines the probabilty the agent chooses a random action. A higher epsilon encourages more exploration, while a lower epsilon emphasizes exploitation of the current best-known actions.ent knowledge.\n",
    "\n",
    "The epsilon parameter is decayed over time during training. This means that, as the agent gains more experience, it tends to rely more on exploitation and less on exploration. The idea is that, as the agent learns and becomes more confident in its estimates, it gradually reduces the rate of exploration. \n",
    "\n",
    "Epsilon initailization is made with the argument **exploration_prob**. It is a list with the lowest and the highest probability values. The probability will decrease with a rate given by the argument **decrease_prob_exp**. By default epsilon is modeled with a linear decay but you can change it to a exponential decay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b309bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(RL.get_epsilon(decay_type = 'linear')[0:50], label = \"linear decay\")\n",
    "plt.plot(RL.get_epsilon(decay_type = 'exponential')[0:50], label = \"exponential decay\")\n",
    "plt.title(\"Probability to choose a random action\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"# episode\")\n",
    "plt.ylabel(\"Epsilon value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8234ff",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">num_episodes, run_limit and convergence_criterion</span>\n",
    "\n",
    "The last three options are hyperparameters. They impact computation time. **num_episodes** is the maximum number of epochs used for training. **run_limit** controls the maximum number of iterations before stopping one episode. **convergence_criterion** is the threshold (difference score between two iterations) to determine convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db459391",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL.q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da876bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL.plot_convergence()\n",
    "RL.plot_convergence(start = 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074dd73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(RL.env.all_states())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85511f9d",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Optimal trajectory</span>\n",
    "\n",
    "Lets see how the rocket evolves after train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe087bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load policy table\n",
    "policy = RL.q_table.copy().replace(0, np.nan)\n",
    "# load environnment\n",
    "env = Environment(JOSN_file, check_model = False)\n",
    "state = env.state_for_q_table()\n",
    "flag_continue = True\n",
    "while flag_continue:\n",
    "    # control while loop\n",
    "    if not policy.index.isin([str(state)]).any():\n",
    "        flag_continue = False\n",
    "        print(\"stop: no moore state\")\n",
    "        continue\n",
    "    if env.m_fuel[-1] < 0:\n",
    "        print(\"stop: no moore fuel\")\n",
    "        env.delete_last_state()\n",
    "        flag_continue = False\n",
    "    action = RL.call_choose_action(state, 0)\n",
    "    _, _, _, _, _ = env.step(action)\n",
    "    state = env.state_for_q_table()\n",
    "\n",
    "dt = pd.DataFrame(env.all_states())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5731bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
