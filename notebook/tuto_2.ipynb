{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a7c5c-5ee2-4210-aec8-521c672a54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "import plotly.express as px \n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import plotly.express as px \n",
    "import pickle \n",
    "sys.path.insert(1, \"../src/utils/\")\n",
    "from agent import Environment\n",
    "from Q_learning import QLearningTrainer\n",
    "sys.path.insert(1, \"plotly_graph/\")\n",
    "from functions4tuto import rocket_simulation, plot_rocket_altitude, plot_reward_rocket_monoagent\n",
    "JOSN_file = \"params/rocket_tuto_2.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ffc6d0",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Input file</span>\n",
    "\n",
    "In the previous tutorial we defined our environment and designed a reward function. The system is a rocket that falling from a starting point and the goal is to reach a target with a minimum speed and maximize fuel.\n",
    "Variable's names, their initial values and boundaries limit are defined into a JSON file (or can be given directly into a dictionary)\n",
    "\n",
    "## <span style=\"color:orange\">Variables</span>\n",
    "\n",
    "Variables can be categorize into 3 classes:\n",
    "* `states_variables` : variable used as coordinate to describe our system\n",
    "* `agent_variables` : variable use as agent. Their values are changed for each iteration\n",
    "* 3th category are other variables. They are not used to describe our environment but they can be usefull to monitor information or to compute intermediate value. \n",
    "  There is no key for this kind of variable. Consider them as variables present into `initial_values` field and that are not `states_variables` and `agent_variables`\n",
    "\n",
    "You can access to the name of state and agent variables, with the attibute `states_variables` and `agent_variables`.\n",
    "\n",
    "    \"states_variables\" : [\"pos_y\", \"acceleration_y\", \"speed_y\"],\n",
    "    \"agent_variables\" : [\"booster\"]\n",
    "\n",
    "## <span style=\"color:orange\">Initial system</span>\n",
    "\n",
    "After to name `states_variables` and `agent_variables`, next step is to define initial state.\n",
    "It will be use as environment coordinates at the beginning of each episode. Initially, it comprises \n",
    "the values of state_variables, followed by agent_variables, and finally other variables that are \n",
    "not used for the system's coordinates.\n",
    "\n",
    "    \"initial_values\" : {\n",
    "      \"pos_y\" : [175.0],\n",
    "      \"speed_y\": [0.0],\n",
    "      \"acceleration_y\": [0.0],\n",
    "      \"weight_rocket\" : [305],\n",
    "      \"booster\" : [0.0],\n",
    "      \"m_fuel\" : [300],\n",
    "      ...\n",
    "      \"ratio_fuel\" : [1.0],\n",
    "      \"dt\" : [3],\n",
    "      \"time\" : [0],\n",
    "      \"acceleration_limit_y\" : [10],\n",
    "      \"speed_limit_y\" : [5]\n",
    "      }\n",
    "\n",
    "**NB:** Because we do not modify rocket's angle, we will not show ```pos_x, speed_x and acceleration_x (you can see all variable by calling env.json[\"initial_values\"])```\n",
    "\n",
    "## <span style=\"color:orange\">Limit</span>\n",
    "\n",
    "Q-learning algorithms model events as a Markov process. Therefore, it is necessary to discretize our environment space. \n",
    "We define lower and upper bounds, as well as the number of divisions we want to use to discretize the variable space.\n",
    "\n",
    "    \"limit\" : {\n",
    "      \"pos_y\" : [0.0, 200.0, 41],\n",
    "      \"speed_y\": [-30.0, 30.0, 31],\n",
    "      \"acceleration_y\": [-20.0, 20.0, 21],\n",
    "      \"weight_rocket\" : [0.0, 305, 62],\n",
    "      \"booster\" : [0.0, 2.0, 3],\n",
    "      \"m_fuel\" : [0.0, 400, 801]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4b678",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Agents's actions</span>\n",
    "    \"n_action\" : {\n",
    "      \"booster\": {\"0\" : 0.0, \"1\" : 1.0, \"2\" : 2.0}\n",
    "    }\n",
    "\n",
    "After defining the variables and their initial values, we proceed to define actions that apply to the agent variables. In this example, we have 1 agent that can take 3 actions:\n",
    "\n",
    "For the booster:\n",
    "  * \"0\": Booster is off.\n",
    "  * \"1\": Booster is turned on to half of its power.\n",
    "  * \"2\": Booster is turned on to its full power.\n",
    "\n",
    "## <span style=\"color:orange\">Actions to take</span>\n",
    "\n",
    "    \"action_to_take\" : {\n",
    "      \"booster\": {\"$booster$\" : \"$action}\n",
    "    }\n",
    "\n",
    "Actions change the agent variables by modifying their values based on the action taken, which are retrieved from the n_action dictionary. You can change default delimiter during the initilisation:\n",
    "\n",
    "`agent = Environment(json_file, delimiter = \"Char_you_want\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba311d",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">System's evolution and reward</span>\n",
    "\n",
    "Last fields are how variables evolve after agents's action and how reward are computed.\n",
    "Only variables present in field inital value are stored. Other variables present are just temporary and are lost after each iteration.\n",
    "Reward values are stored into a dictionnary, the keys are agent variable's name.\n",
    "Timestep value (dt) is 3, to let the system evolve enough without the need to have an high resolution in state variable's space.\n",
    "\n",
    "### <span style=\"color:orange\">Equation variables</span>\n",
    "    \"equations_variables\": {\n",
    "        \"$F$\" : \"600\",\n",
    "        \"$y_0$\" : \"$pos_y$\",\n",
    "        \"$Vy_0$\" : \"$speed_y$\",\n",
    "        \"$m_fuel$\" : \"$m_fuel$ - $booster$ *5 -np.ceil( np.abs($alpha$) ) *5\",\n",
    "        \"$weight_rocket$\" : \"$weight_dry_rocket$ + $m_fuel$\",\n",
    "        \"$acceleration_y$\" : \"($F$/(5+$weight_rocket$) * np.cos($angle$)) * $booster$ - $G$\",\n",
    "        \"$speed_y$\" : \"($F$/(5+$weight_rocket$) * np.cos($angle$)) * $booster$ * $dt$ - $G$ * $dt$ + $Vy_0$\",\n",
    "        \"$pos_x$\": \"(0.5 * $F$/(5+$weight_rocket$) * np.sin($angle$)) * $booster$ * $dt$**2 + $Vx_0$ * $dt$ + $x_0$\",\n",
    "        \"$pos_y$\": \"(0.5 * $F$/(5+$weight_rocket$) * np.cos($angle$)) * $booster$ * $dt$**2 - $G$ * $dt$**2 + $Vy_0$ * $dt$ + $y_0$\",\n",
    "        \"$futur_pos_y$\" : \"$pos_y$ + 3 * $speed_y$\",\n",
    "        \"y_lower_limit\" : \"0\",\n",
    "        \"y_upper_limit\" : \"200\",\n",
    "        \"$upper_boundary$\": \"-np.exp(0) + np.exp(np.min([ np.min(-$futur_pos_y$ + y_upper_limit), 0]))\",\n",
    "        \"$lower_boundary$\": \"-np.exp(0) + np.exp(np.min([ np.min($futur_pos_y$ -y_lower_limit), 0]))\"\n",
    "    },\n",
    "\n",
    "### <span style=\"color:orange\">Reward</span>\n",
    "\n",
    "The reward indicates the immediate benefit or cost associated with the action.\n",
    "The scalar feedback signal that the environment sends to the agent after it takes an action are defined into this dictionnary.\n",
    "\n",
    "    \"equations_rewards\": {\n",
    "      \"$booster$\" : \"2*(-distance_y_reward) + speed_y_reward + 0.5 * ratio_fuel\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9883a51a",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Stop episode</span>\n",
    "\n",
    "stop episode when goal is reach. If feature has 1 value, its feature's value must be equal.\n",
    "In other hand, if feature has 2 values ([min_limit, max_limit]), criterion is bounded feature >= min_limit and feature <= max_limit\n",
    "\n",
    "    \"stop_episode\" : {\n",
    "      \"pos_y\" : [0, 5],\n",
    "      \"acceleration_y\" : [-2,2],\n",
    "      \"speed_y\" : [-2,2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edae6973",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">A simple scenario</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84614d8e",
   "metadata": {},
   "source": [
    "We want to land our rocket without it crashing. In this scenario, the rocket can take three actions: turn off its booster, or turn it on at half or full power.\n",
    "\n",
    "Let’s model our ideal scenario. We want to activate the booster when the rocket exceeds speed or acceleration limits, and on the other hand, we want to conserve fuel whenever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d7405-5fcf-45ee-8abf-c1af457b58b9",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Intialize environment</span>\n",
    "\n",
    "Create an environment object with the rules defined previously. The idea is to check the new field can raise a flag to stop simulation when the criteria are reached:\n",
    "\n",
    "* 0 <= pos_y <= 5\n",
    "* -2 <= acceleration_y <= 2\n",
    "* -8 <= speed_y <= 8\n",
    "\n",
    "We will also control the speed of the rocket and its acceleration to keep as possible those parameters between this range:\n",
    "\n",
    "* -5 >= speed_y <= 5\n",
    "* -10 <= acceleration_y <= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment object with the rules defined previously\n",
    "env_ref = Environment(JOSN_file, check_model = True)\n",
    "df_0 = rocket_simulation(env_ref, acceleration_y_constraint = 10, speed_y_limit = 5, timestep = 0)\n",
    "plot_rocket_altitude(df_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978f594d",
   "metadata": {},
   "source": [
    "The altitude plot shows that at t = 18, the rocket's altitude is 12.5, but its speed is too high. In fact, at the next timestep, the rocket's position drops to -7.4, indicating a crash.\n",
    "\n",
    "The most important factor here is the speed: it exceeds the desired range of [-5, 5]. Unlike speed, the acceleration remains within acceptable limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a95135",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_0.time, df_0.speed_y, \"+-\", label=\"Rockect's speed\")\n",
    "plt.title(\"Rocket's speed\")\n",
    "plt.hlines(-5, df_0.time.to_numpy()[0], df_0.time.to_numpy()[-1], \n",
    "           color = \"red\", linestyles = 'dashdot', label=\"desired speed limit\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Speed\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df_0[[\"time\", \"pos_y\", \"speed_y\", \"acceleration_y\", \"m_fuel\", \"booster\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54146bb",
   "metadata": {},
   "source": [
    "The speed and final position do not fully meet the success criteria.\n",
    "To avoid a crash (where the speed is too high to be corrected in the next timestep), we could use the future position as an additional criterion.\n",
    "The future position is calculated using the current speed and the timestep, according to the formula:\n",
    "\n",
    "$$ futur\\_positiion = pos_y + speed_y * timestep $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73085b22",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Test different timestep for futur position</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4752616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different timestep to compute furtur rocket position\n",
    "for timestep in [3, 5, 10]:\n",
    "    df_0[\"futur_position_dt+{0}\".format(timestep)] = df_0[\"pos_y\"] + df_0[\"speed_y\"] * timestep\n",
    "plot_rocket_altitude(df_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d175966d",
   "metadata": {},
   "source": [
    "Let's examine the rocket's landing. As you can see, at t = 21, its position is -7.4, indicating a crash.\n",
    "Ideally, we would want to prevent the crash by activating the engine before it's too late.\n",
    "\n",
    "Using the future position projected at dt+3 or dt+5, the model predicts the crash only one timestep before it actually happens, which is too late to correct the rocket's trajectory.\n",
    "\n",
    "On the other hand, dt+10 predicts the crash three timesteps earlier, providing enough time to react. This makes it a good candidate for implementing a static decision rule to avoid crashes.\n",
    "\n",
    "However, depending on the rocket's speed and acceleration, using dt+10 can be problematic. If the rocket is already close to the ground and moving at low speed with low acceleration, the future position at dt+10 might incorrectly suggest a crash, causing the engine to turn on unnecessarily.\n",
    "\n",
    "For the reason, we will use reinforcement learning to learn landing correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d8ce7",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Rewards</span>\n",
    "\n",
    "Before to use reinforcement learning. Let's see how each global reward and its individual components (without their weighting coefficients) evolve. As a reminder, the reward formula for the booster is:\n",
    "\n",
    "$$  -2 * distance\\_y\\_reward + speed\\_y\\_reward + 0.5 * ratio\\_fuel $$ \n",
    "\n",
    "We know that in our first attempt, the rocket did not respect the speed limitation.\n",
    "Let’s check whether this undesired behavior is properly detected and penalized by the reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6c6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_ref.reset()\n",
    "df_0 = rocket_simulation(env_ref, acceleration_y_constraint = 10, speed_y_limit = 5, timestep = 0)\n",
    "plot_reward_rocket_monoagent(env_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1181503",
   "metadata": {},
   "source": [
    "As we can see, when the rocket's speed is out of the desired range, the speed reward is negatively impacted, and its influence becomes stronger—even as the rocket gets closer to the ground.\n",
    "Only when the rocket's speed returns to within the acceptable range does the speed penalty drop back to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc9cc0",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Reinforcemet learning</span>\n",
    "\n",
    "\n",
    "After load our environment, next step is to create QLearningTrainer objet. It will apply Qlearning algorithm. For each states, the algorithm will apply a score base to the next iteration.\n",
    "\n",
    "The Bellman equation is the value function use in reinforcement learning. \n",
    "\n",
    "$V(s,a) = V(s, a)+ \\alpha * (R+\\gamma * max(V(s', a')) - V(s, a))$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $V(s, a)$ is the estimated value of state $s$ and actions $a$\n",
    "* $s$ is the curent state\n",
    "* $s'$ is the next state\n",
    "* $a$ is action taken\n",
    "* $a'$ next possible actions that maximize reward\n",
    "* $R$ is the immediate reward received after transitioning\n",
    "* $\\gamma$ is the discount factor, which determines the importance of future rewards.\n",
    "* $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c07191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RL object\n",
    "env = Environment(JOSN_file, check_model = True)\n",
    "RL = QLearningTrainer(env, \n",
    "                      num_episodes = 1000, \n",
    "                      convergence_criterion = 0.5, \n",
    "                      decay_type = \"exponential\",\n",
    "                      decrease_prob_exp = 0.015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d31355",
   "metadata": {},
   "source": [
    "Different parameters are avaible. For the tutorial, you use default parameters. Before to lunch training. Lets discus about the input parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c9872",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Learning rate</span>\n",
    "\n",
    "\n",
    "The learning rate $\\alpha$ in the Bellman equation controls the weight given to the new estimate compared to the existing estimate of the state value. A smaller learning rate means that the new estimate has less influence, and the agent is more conservative in updating its value function. A larger learning rate allows the agent to adjust its estimates more rapidly based on new information\n",
    "\n",
    "### <span style=\"color:orange\">Discount factor (gamma)</span>\n",
    "\n",
    "The discount factor $\\gamma$, is a key hyperparameter that controls how much the agent values future rewards compared to immediate rewards. For rocket landing, $\\gamma$ equal to 0.99, since we are optimizing over multiple steps to reach a safe landing.\n",
    "\n",
    "### <span style=\"color:orange\">Epsilon parameter</span>\n",
    "\n",
    "Exploration-exploitation is a fundamental trade-off in reinforcement learning, where the agent needs to balance between exploring new actions and exploiting the knowledge it has gained so far.\n",
    "\n",
    "The epsilon-greedy policy is a simple strategy that the agent uses to decide whether to explore a new action (random exploration) or exploit the current best-known action. It helps prevent the agent from getting stuck in suboptimal policies by occasionally trying new actions. The value of epsilon determines the probabilty the agent chooses a random action. A higher epsilon encourages more exploration, while a lower epsilon emphasizes exploitation of the current best-known actions.ent knowledge.\n",
    "\n",
    "The epsilon parameter is decayed over time during training. This means that, as the agent gains more experience, it tends to rely more on exploitation and less on exploration. The idea is that, as the agent learns and becomes more confident in its estimates, it gradually reduces the rate of exploration. \n",
    "\n",
    "Epsilon initailization is made with the argument **exploration_prob**. It is a list with the lowest and the highest probability values. The probability will decrease with a rate given by the argument **decrease_prob_exp**. By default epsilon is modeled with a linear decay but you can change it to a exponential decay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b309bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(RL.get_epsilon(decay_type = 'linear')[0:400], label = \"linear decay\")\n",
    "plt.plot(RL.get_epsilon(decay_type = 'exponential')[0:400], label = \"exponential decay\")\n",
    "plt.title(\"Probability to choose a random action\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"# episode\")\n",
    "plt.ylabel(\"Epsilon value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8234ff",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">num_episodes, run_limit and convergence_criterion</span>\n",
    "\n",
    "The last three options are hyperparameters that impact computation time:\n",
    "* ``num_episodes``: The maximum number of training episodes.\n",
    "* ``run_limit``: The maximum number of iterations allowed within a single episode before it is forcefully stopped.\n",
    "* ``convergence_criterion``: The threshold used to determine convergence, based on the difference in Q-values between two iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b46904f",
   "metadata": {},
   "source": [
    "Before launching the training, it's important to understand that the Q-learning algorithm operates in a discretized space.\n",
    "The spacing between bins can significantly impact convergence. If a discretized bin groups together too many different underlying states, the Q-table may become unstable, making it difficult for the algorithm to converge.\n",
    "\n",
    "Let's examine how our space is discretized using ``RL.env.json[\"limit\"]``.\n",
    "The agent variable (booster) has 3 bins, while the state variables \"pos_y\", \"acceleration_y\", \"speed_y\", and \"weight_rocket\" have 41, 31, 21, and 62 bins, respectively.\n",
    "\n",
    "This results in a Q-table with:\n",
    "Our Q table is composed of $$ 3 * 41 *31 *21 *62 = 4 964 526\\  states $$\n",
    "\n",
    "Does that mean our system will visit every possible state?\n",
    "Of course not. Remember, the Q-learning algorithm starts with a random exploration (a random walk), but over time, the agent’s exploration becomes guided by what it has already experienced.\n",
    "As a result, some states may never be visited if the agent never finds a path leading to them or if they are not useful for improving the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1e25d2",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Training</span>\n",
    "\n",
    "Training with our parameters should take between 8 and 10 minutes. You can also load a pre-trained RL object (trained ouput are stored in ``saved_model/output_train_monoagent.txt``). If you want to generate the model from scratch, start training with the command: ``RL.q_learning()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model/RL_monoagent.pkl', 'rb') as f:\n",
    "    RL = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db459391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start training remove comment\n",
    "# RL.q_learning()\n",
    "# #save model\n",
    "# with open('saved_model/RL_monoagent.pkl', 'wb') as f:  # open a text file\n",
    "#     pickle.dump(RL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da876bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL.plot_convergence() \n",
    "RL.plot_convergence(start = len(RL.loss_episodes) -10) #loss for the last 10 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b0531",
   "metadata": {},
   "source": [
    "If you loaded the pre-trained model, you'll see that training stopped at episode 677. For the last five iterations the difference between $Qtable_N - Qtable_{N-1}$ remained bellow the convergence threshold (convergence_criterion = 0.5), indicating that the learning process had stabilized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c25ab48",
   "metadata": {},
   "source": [
    "Let’s find out how many states we actually explored.\n",
    "To do this, we replace Q-table values equal to 0 with np.nan.\n",
    "(Except for goal-reaching states in our setup, a value of 0 typically means the state was never explored.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffec4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_explored = np.sum(RL.q_table.copy().replace(0, np.nan).count())\n",
    "\n",
    "print(\"Number of states explored {0}.\\\n",
    " Only {1:.3f}% states have been visited\".format(\n",
    "    states_explored,\n",
    "    states_explored/4964526 *100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85511f9d",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Optimal trajectory</span>\n",
    "\n",
    "Let’s see how the rocket evolves after training.\n",
    "We start by copying the Q-table using the ``q_table`` attribute, and we replace all 0 values with ``np.nan`` to better visualize learned values.\n",
    "\n",
    "Next, we load a new environment and use its current discretized state as the starting point.\n",
    "At each iteration, we follow the action suggested by the Q-table based on the environment’s current discretized state, which is updated after every step. At the end, we save all the states visited into a data frame (dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load policy table\n",
    "policy = RL.q_table.copy().replace(0, np.nan)\n",
    "# load environnment\n",
    "env = Environment(JOSN_file, check_model = False)\n",
    "state = env.state_for_q_table()\n",
    "flag_continue = True\n",
    "while flag_continue:\n",
    "    # control while loop\n",
    "    if not policy.index.isin([str(state)]).any():\n",
    "        flag_continue = False\n",
    "        print(\"stop: no moore state\")\n",
    "        env.delete_last_states()\n",
    "        continue\n",
    "    elif env.pos_y[-1] < 0:\n",
    "        print(\"stop: reach goal\")\n",
    "        env.delete_last_states()\n",
    "        flag_continue = False\n",
    "        continue\n",
    "    action = RL.call_choose_action(state, 0)\n",
    "    _, rewards, done, problem, info = env.step(action)\n",
    "    state = env.state_for_q_table()\n",
    "\n",
    "dt = pd.DataFrame(env.all_states())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c671cc",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Rocket Trajectory Comparison</span>\n",
    "\n",
    "Let’s compare the initial rocket landing with the new one obtained using our trained policy.\n",
    "As we can see, the second rocket reaches the ground more slowly than the first. Its trajectory appears more linear, and if we look at the speed profile, the reinforcement learning model keeps the speed within the desired limit of [-5, 5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3529b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_0[[\"time\"]], df_0[[\"pos_y\"]], label = \"intial trajectory\")\n",
    "plt.plot(dt[[\"time\"]], dt[[\"pos_y\"]], label = \"RL model trajectory\")\n",
    "plt.plot(dt[[\"time\"]], np.array([-5]*len(dt[[\"time\"]])), label = \"Ground\" )\n",
    "plt.title(\"Rocket landing\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Altitude\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(df_0[[\"time\"]], df_0[[\"speed_y\"]], \"+-\",label = \"intial model speed\")\n",
    "plt.plot(dt[[\"time\"]], dt[[\"speed_y\"]], \"+-\", label = \"RL model speed\")\n",
    "plt.plot(dt[[\"time\"]], np.array([-5]*len(dt[[\"time\"]])), label = \"Speed limit\" )\n",
    "plt.title(\"Rocket speed\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Speed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343a8d8",
   "metadata": {},
   "source": [
    "Even though the second rocket consumes more fuel, it successfully keeps the speed within the desired range.\n",
    "As shown in the reward plots, the RL policy allows the rocket to reach its goal more smoothly, with better speed control compared to the first attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5af9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_rocket_monoagent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe267d",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Conclusion</span>\n",
    "\n",
    "With our designed reward function and discretization settings, we provide a model capable of landing the rocket with a safe speed. In the next chapter, we will se how to use 2 agents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
