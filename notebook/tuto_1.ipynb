{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a7c5c-5ee2-4210-aec8-521c672a54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "import plotly.express as px \n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import plotly.express as px \n",
    "sys.path.insert(1, \"../src/utils/\")\n",
    "from agent import Environment\n",
    "sys.path.insert(1, \"plotly_graph/\")\n",
    "from functions4tuto import plotly_trajectory, control_fall_simulation, booster_reward, plotly_all_reward\n",
    "JOSN_file = \"params/rocket_tuto_1.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ffc6d0",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Input file</span>\n",
    "\n",
    "Before to start trainning we need to define our environment. In this example, the system is a rocket. \n",
    "It falls from a starting point and the goal is to reach a target with a minimum speed and maximize fuel.\n",
    "Variable's names, their initial values and boundaries limit are defined into a JSON file (or can be given directly into a dictionary)\n",
    "\n",
    "## <span style=\"color:orange\">Variables</span>\n",
    "\n",
    "Variables can be categorize into 3 classes:\n",
    "* `states_variables` : variable used as coordinate to describe our system\n",
    "* `agent_variables` : variable use as agent. Their values are changed for each iteration\n",
    "* 3th category are other variables. They are not used to describe our environment but they can be usefull to monitor information or to compute intermediate value. \n",
    "  There is no key for this kind of variable. Consider them as variables present into `initial_values` field and that are not `states_variables` and `agent_variables`\n",
    "\n",
    "You can access to the name of state and agent variables, with the attibute `states_variables` and `agent_variables`.\n",
    "\n",
    "    \"states_variables\" : [\"pos_y\", \"acceleration_y\", \"speed_y\"],\n",
    "    \"agent_variables\" : [\"booster\"]\n",
    "\n",
    "## <span style=\"color:orange\">Initial system</span>\n",
    "\n",
    "After to name `states_variables` and `agent_variables`, next step is to define initial state.\n",
    "It will be use as environment coordinates at the beginning of each episode. Initially, it comprises \n",
    "the values of state_variables, followed by agent_variables, and finally other variables that are \n",
    "not used for the system's coordinates.\n",
    "\n",
    "    \"initial_values\" : {\n",
    "      \"pos_y\" : [175.0],\n",
    "      \"acceleration_y\": [0.0],\n",
    "      \"speed_y\": [0.0],\n",
    "      \"angle\" : [0.0],\n",
    "      \"booster\" : [0.0],\n",
    "      \"alpha\" : [0.0],\n",
    "      \"futur_pos_y\" : [175.0],\n",
    "      \"m_fuel\" : [100],\n",
    "      \"weight_rocket\" : [105],\n",
    "      \"weight_dry_rocket\" : [5],\n",
    "      \"G\" : [1.62],\n",
    "      \"m_fuel_ini\" : [100.0],\n",
    "      \"pos_y_star\": [0.0]\n",
    "      }\n",
    "\n",
    "\n",
    "## <span style=\"color:orange\">Limit</span>\n",
    "\n",
    "Q-learning algorithms model events as a Markov process. Therefore, it is necessary to discretize our environment space. \n",
    "We define lower and upper bounds, as well as the number of divisions we want to use to discretize the variable space.\n",
    "\n",
    "\n",
    "    \"limit\" : {\n",
    "      \"pos_y\" : [0.0, 300.0, 61],\n",
    "      \"acceleration_y\": [-20.0, 20.0, 21],\n",
    "      \"speed_y\": [-50.0, 50.0, 21],\n",
    "      \"angle\" : [-0.8, 0.8, 17],\n",
    "      \"booster\" : [0.0, 1.0, 3],\n",
    "      \"alpha\" : [-0.1, 0.1, 3],\n",
    "      \"m_fuel\" : [0.0, 100, 101]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4b678",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Agents's actions</span>\n",
    "    \"n_action\" : {\n",
    "      \"booster\": {\"0\" : 0.0, \"1\" : 0.5, \"2\" : 1.0}\n",
    "    }\n",
    "\n",
    "After defining the variables and their initial values, we proceed to define actions that apply to the agent variables. In this example, we have 1 agent that can take 3 actions:\n",
    "\n",
    "For the booster:\n",
    "  * \"0\": Booster is off.\n",
    "  * \"1\": Booster is turned on to half of its power.\n",
    "  * \"2\": Booster is turned on to its full power.\n",
    "\n",
    "## <span style=\"color:orange\">Actions to take</span>\n",
    "\n",
    "    \"action_to_take\" : {\n",
    "      \"booster\": {\"$booster$\" : \"$action}\n",
    "    }\n",
    "\n",
    "Actions change the agent variables by modifying their values based on the action taken, which are retrieved from the n_action dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba311d",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">System's evolution and reward</span>\n",
    "\n",
    "Last fields are how variables evolve after agents's action and how reward are computed.\n",
    "Only variables present in field inital value are stored. Other variables present are just temporary and are lost after each iteration.\n",
    "Reward values are stored into a dictionnary, the keys are agent variable's name. \n",
    "\n",
    "### <span style=\"color:orange\">Equation variables</span>\n",
    "\n",
    "    \"equations_variables\": {\n",
    "        \"$F$\" : \"600\",\n",
    "        \"$m_fuel$\" : \"$m_fuel$ - $booster$ *10 -$angle$ *10\",\n",
    "        \"$weight_rocket$\" : \"$weight_dry_rocket$ + $m_fuel$\",\n",
    "        \"dt\" : \"0.5\",\n",
    "        \"$theta$\" : \"0.0\",\n",
    "        \"$y_0$\" : \"$pos_y$\",\n",
    "        \"$Vy_0$\" : \"$speed_y$\",\n",
    "        \"$angle$\" : \"$theta$ + $alpha$\",\n",
    "        \"$acceleration_y$\" : \"($F$/(5+$weight_rocket$) * np.cos($angle$)) * $booster$ - $G$\",\n",
    "        \"$speed_y$\" : \"($F$/(5+$weight_rocket$) * np.cos($angle$)) * $booster$ * $dt$ - $G$ * $dt$ + $Vy_0$\",\n",
    "        \"$pos_y$\": \"(0.5 * $F$/(5+$weight_rocket$) * np.cos($angle$)) * $booster$ * $dt$**2 - $G$ * $dt$**2 + $Vy_0$ * $dt$ + $y_0$\",\n",
    "        \"$futur_pos_y$\" : \"$pos_y$ + 3 * $speed_y$\"\n",
    "    },\n",
    "\n",
    "### <span style=\"color:orange\">Reward</span>\n",
    "\n",
    "The reward indicates the immediate benefit or cost associated with the action.\n",
    "The scalar feedback signal that the environment sends to the agent after it takes an action are defined into this dictionnary.\n",
    "\n",
    "    \"equations_rewards\": {\n",
    "      \"$booster$\" : \"-($pos_y$ - $pos_y_star$)**2 + $m_fuel$/$m_fuel_ini$\"\n",
    "    }\n",
    "\n",
    "### <span style=\"color:orange\">stop_episode</span>\n",
    "\n",
    "The optional `stop_episode` field stops episode when system reaches a desired goal.\n",
    "You can give on or many features and specify stop condition.\n",
    "If feature has 1 value, the value must be equal. In other hand, if feature has 2 values [min_limit, max_limit]. Criterion is bounded feature >= min_limit and feature <= max_limit.\n",
    "\n",
    "\n",
    "    \"stop_episode\" : {\n",
    "      \"pos_y\" : [1, 0],\n",
    "      \"acceleration_y\" : [-2,2],\n",
    "      \"speed_y\" : [-1,1]\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430d0744",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Lets see how system evolves</span>\n",
    "\n",
    "A premilary work must be done on reward function. Indeed, we have 2 agents. Each one has an impact on rocket trajectory. \n",
    "\n",
    "Lets simulate a simple case. Our rocket start with no speed. The only force applied on it is G (Gravitational constant). \n",
    "The rocket falls down straight on the planetoid (angle is zero no needs to correct it) and without friction. The goal is to reach a point (pos_x_star, pos_y_star).\n",
    "\n",
    "We start booster engine when we are close to the ground. For that, we compute the new position after 3 * dt (`futur_pos_y`).\n",
    "When `futur_pos_y` is bellow zero, the rocket activates engine to compensate the fall speed and avoid the crash. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0eaf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment object with the rules defined previously\n",
    "env = Environment(JOSN_file, check_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e92c10-31d1-4afa-bd6b-ca0fd8e287bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment object with the rules defined previously\n",
    "env = Environment(JOSN_file, check_model = False)\n",
    "flag = \"0\"\n",
    "flag_to_continue = True\n",
    "# monitor action takes for each iteration\n",
    "actions = {\"action_booster\" : []} \n",
    "while flag_to_continue:    \n",
    "    _, rewards, done, problem, info = env.step([flag])\n",
    "    actions[\"action_booster\"].append(flag)\n",
    "    if env.futur_pos_y[-1] <= 0 and env.m_fuel[-1] > 0:\n",
    "        flag = \"2\"\n",
    "    # stop engine if there is no fuel\n",
    "    if env.m_fuel[-1] == 0:\n",
    "        flag = \"0\"\n",
    "    # stop simulation\n",
    "    if env.pos_y[-1] < 0:\n",
    "        flag_to_continue = False\n",
    "        # delete last state because rocket is bellow to the ground\n",
    "        env.delete_last_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9105f1c3",
   "metadata": {},
   "source": [
    "Each state of our system is saved in `env`. We can access the last state of our system using the method `last_state()`.\n",
    "If we want to access a specific range of states, it is the method `last_state()` (by default, all states are loaded)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb4ecf",
   "metadata": {},
   "source": [
    "Lets visualize how the system evolves !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fffddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save result into a pandas data frame and add column for time\n",
    "df_traj = pd.concat([pd.DataFrame(env.all_states()), pd.DataFrame(actions)], axis = 1)\n",
    "df_traj[\"iter\"] = np.arange(0, df_traj.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c824d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_trajectory(df_traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edffef93",
   "metadata": {},
   "source": [
    "At t=0, the rocket is held in the air and its velocity is 0. Once released, the only force acting on the rocket is gravity. \n",
    "Therefore, our rocket will accelerate constantly and gain speed. At t=24, the projection of the rocket's position is below 0. \n",
    "This means that the rocket will reach the ground in 3 time steps if nothing is done. To avoid catastrophe, the boosters are activated. \n",
    "The boosters will provide acceleration that counteracts the gravitational constant. \n",
    "Therefore, as the rocket burn fuel its weight decreases and the acceleration increases and the rocket gains speed in the opposite direction of its fall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2122187c",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Design reward functions</span>\n",
    "\n",
    "Let's visualize the evolution of the reward functions by using the attribut `reward`. As a reminder, the reward function for the booster corresponds to the vertical distance relative to pos_y_star plus the fuel ratio.\n",
    "While for alpha (which corresponds to the angle of the rocket), its reward function corresponds to the horizontal distance relative to pos_x_star, minus the rocket's tilt angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ef607",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(pd.DataFrame(env.rewards),\n",
    "        labels=dict(index=\"time\", value=\"Reward\", variable=\"Agent\") )\n",
    "fig.update_layout(height=400, width=600, title_text=\"Agent's reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc9cc0",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Constraint acceleration and speed</span>\n",
    "\n",
    "\n",
    "The current reward function for the booster is far from optimal. As evident from the data, the value increases steadily and peaks at the 26th iteration. However, the engine won't start until then, risking a crash. Instead of using `pos_y`, we can utilize `future_pos_y` to allow ample time to start the engine and prevent a collision. While predicting the next position after a 3-time step works well in this example, in other scenarios, the rocket's speed may be excessive, rendering 3 time steps insufficient. To mitigate such scenarios, we could also impose constraints on the vehicle's acceleration and speed.\n",
    "\n",
    "We can modulate speed in a way, system will be high penalize when it begans to be out of bounds:\n",
    "\n",
    "$$  \\exp(1) - \\exp(Max( \\frac{|speed\\_y|}{speed\\_y\\_limit} , 1)) $$ \n",
    "\n",
    "\n",
    "\n",
    "The same reasoning can be applied to acceleration.\n",
    "\n",
    "$$  \\exp(1) - exp(Max( \\frac{|acceleration\\_y|}{acceleration\\_y\\_limit} , 1)) $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf516dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_y_limit = 10\n",
    "speed_y = np.arange(-15,16)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = speed_y,\n",
    "    y = np.array([ np.exp(1) - np.exp(np.max([val, 1])) for val in np.abs(speed_y)/speed_y_limit ]),\n",
    "    name='Speed constraint',\n",
    "    mode='lines+markers'\n",
    "))\n",
    "acceleration_y_limit = 5\n",
    "acceleration_y = np.arange(-11,12)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = acceleration_y,\n",
    "    y = np.array([ np.exp(1) - np.exp(np.max([val, 1])) for val in np.abs(acceleration_y)/acceleration_y_limit ]),\n",
    "    name='Acceleration constraint',\n",
    "    mode='lines+markers'\n",
    "))\n",
    "\n",
    "fig.update_xaxes(title_text = \"Speed or acceleration\")\n",
    "fig.update_yaxes(title_text = \"Penalty\")\n",
    "fig.update_layout(height=400, width=600, title_text=\"Function to penalize speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb5c07d",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Height boundaries for furtur pos y</span>\n",
    "\n",
    "\n",
    "After we've imposed constraints on acceleration and speed, the final step is to discourage the system from crashing. \n",
    "To achieve this, we set a minimum bound for the height.\n",
    "\n",
    "$$ -\\exp(0) + \\exp(Min( y\\_lower\\_limit - futur\\_pos\\_y, 0)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc4d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lower_limit = 0\n",
    "futur_pos_y = np.arange(-7, 7)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = futur_pos_y,\n",
    "    y = np.array([-np.exp(0) + np.exp(np.min([val, 0])) for val in futur_pos_y - y_lower_limit ]),\n",
    "    name='height constraint',\n",
    "    mode='lines+markers'\n",
    "))\n",
    "\n",
    "fig.update_xaxes(title_text = \"height\")\n",
    "fig.update_yaxes(title_text = \"Penalty\")\n",
    "fig.update_layout(height=400, width=600, title_text=\"Function to penalize speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb716b1",
   "metadata": {},
   "source": [
    "We can also discourage the system from going too far away. \n",
    "To achieve this, we set a minimum bound for the height.\n",
    "\n",
    "$$ \\exp(0) + \\exp(Min( y\\_upper\\_limit - futur\\_pos\\_y, 0)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_upper_limit = 15\n",
    "futur_pos_y = np.arange(7, 20)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = futur_pos_y,\n",
    "    y = np.array([np.exp(0) + np.exp(np.min([val, 0])) for val in -futur_pos_y + y_upper_limit ]),\n",
    "    name='height constraint',\n",
    "    mode='lines+markers'\n",
    "))\n",
    "\n",
    "fig.update_xaxes(title_text = \"height\")\n",
    "fig.update_yaxes(title_text = \"Penalty\")\n",
    "fig.update_layout(height=400, width=600, title_text=\"Function to penalize speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3067c",
   "metadata": {},
   "source": [
    "By combining the 2 constraint we got:\n",
    "\n",
    "$$ \\exp(Min( y\\_upper\\_limit - futur\\_pos\\_y, 0)) + \\exp(Min( y\\_lower\\_limit - futur\\_pos\\_y, 0)) -2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93cb2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_upper_limit = 15\n",
    "y_lower_limit = 0\n",
    "\n",
    "futur_pos_y = np.arange(-7, 23)\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = futur_pos_y,\n",
    "    y = np.array([-2 + np.exp(np.min([val1, 0])) + np.exp(np.min([val2, 0])) for val1, val2 in zip(futur_pos_y - y_lower_limit, -futur_pos_y + y_upper_limit) ]),\n",
    "    name='height constraint',\n",
    "    mode='lines+markers'\n",
    "))\n",
    "\n",
    "fig.update_xaxes(title_text = \"height\")\n",
    "fig.update_yaxes(title_text = \"Penalty\")\n",
    "fig.update_layout(height=400, width=600, title_text=\"Function to counstraint height exploration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf0a649",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Evaluate penalties function</span>\n",
    "\n",
    "We designed our function to penalize undesired behavior. It's time to simulate what will happen. \n",
    "The system begins from the same starting point, and we add a new condition: if the speed or acceleration is above or below their respective upper and lower limits, the rocket's engine is turned off or on, depending on the situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5295d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt, df_penalty = control_fall_simulation(JOSN_file, \n",
    "                            acceleration_y_constraint = 5, \n",
    "                            speed_y_limit = 17, \n",
    "                            y_lower_limit = 0)\n",
    "# Trajecotry result do not plot event after the last propulsion\n",
    "plotly_all_reward(dt, df_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fdd4f8",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Conclusion</span>\n",
    "\n",
    "We designed each component of our reward function. Initially, the primary component is the distance penalty. As the rocket approaches its endpoint, this penalty decreases further. Similarly, for penalty rewards, when they are out of bounds, they are heavily penalized. The last time the booster was turned on, the acceleration was out of bounds, and the system was heavily penalized; moreover, its speed was too fast at the end. If we only consider the first 70 time steps, our reward function seems good.\n",
    "\n",
    "The final step consists of assigning weights to our components to prevent a value of 0 at the beginning and to give more weight to the distance component and the y_constraint.\n",
    "\n",
    "$$ reward =  3 * dist\\_star + acceleration\\_y\\_constraint + speed\\_y\\_constraint + 2 *y\\_lim\\_constraint +  \\frac{ratio\\_fuel}{2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41855957",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penalty[\"sum_penalty\"] = 3 * df_penalty.dist_star + df_penalty.acceleration_constraint +\\\n",
    "      df_penalty.speed_constraint + 2 * df_penalty.y_lim_constraint +\\\n",
    "          df_penalty.ratio_fuel/2\n",
    "\n",
    "fig = px.line(df_penalty.sum_penalty[:70], title = \"Reward function\",\n",
    "        labels = {\"index\" : \"Time\", \"value\" : \"Reward\"})\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
