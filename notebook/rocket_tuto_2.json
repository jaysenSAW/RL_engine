{
    "states_variables" : ["pos_y",  "acceleration_y", "speed_y"],
    "agent_variables" : ["booster"],
    "initial_values" : {
      "pos_y" : [175.0],
      "acceleration_y": [0.0],
      "speed_y": [0.0],
      "angle" : [0.0],
      "booster" : [0.0],
      "alpha" : [0.0],
      "futur_pos_y" : [175.0],
      "m_fuel" : [100],
      "weight_rocket" : [105],
      "weight_dry_rocket" : [5],
      "G" : [1.62],
      "m_fuel_ini" : [100.0],
      "pos_y_star": [0.0]
      },
    "_limit" : ["min", "max", "n_bins"],
    "limit" : {
      "pos_y" : [0.0, 300.0, 61],
      "acceleration_y": [-20.0, 20.0, 21],
      "speed_y": [-50.0, 50.0, 21],
      "angle" : [-0.8, 0.8, 17],
      "booster" : [0.0, 1.0, 3],
      "alpha" : [-0.1, 0.1, 3],
      "m_fuel" : [0.0, 100, 101]
    },
    "n_action" : {
      "booster": {"0" : 0.0, "1" : 0.5, "2" : 1.0}
    },
    "_action_to_take" : "How action change agent_variables, value is taken from n_action dictionary",
    "action_to_take" : {
      "booster": {"$booster$" : "$action$"}
    },
    "_equations" : "compute values for each equation. Variable are updated after the loop",
    "equations_variables": {
        "$F$" : "600",
        "$m_fuel$" : "$m_fuel$ - $booster$ *10 -$angle$ *10",
        "$weight_rocket$" : "$weight_dry_rocket$ + $m_fuel$",
        "dt" : "0.5",
        "$theta$" : "0.0",
        "$y_0$" : "$pos_y$",
        "$Vy_0$" : "$speed_y$",
        "$angle$" : "$theta$ + $alpha$",
        "$acceleration_y$" : "($F$/(5+$weight_rocket$) * np.cos($angle$)) * $booster$ - $G$",
        "$speed_y$" : "($F$/(5+$weight_rocket$) * np.cos($angle$)) * $booster$ * $dt$ - $G$ * $dt$ + $Vy_0$",
        "$pos_y$": "(0.5 * $F$/(5+$weight_rocket$) * np.cos($angle$)) * $booster$ * $dt$**2 - $G$ * $dt$**2 + $Vy_0$ * $dt$ + $y_0$",
        "$futur_pos_y$" : "$pos_y$ + 3 * $speed_y$"
    },
    "equations_rewards": {
      "speed_limit" : "10",
      "speed_y_reward" : "-np.max([ np.abs( $speed_y$ ) - speed_limit])",
      "acceleration_limit" : "5",
      "acceleration_y_reward" : "-np.max([ np.abs( $acceleration_y$ ) - acceleration_limit])",
      "distance_y_reward" : "-($pos_y$ - $pos_y_star$)**2",
      "y_lower_limit" : "0",
      "height_boundaries" : "-np.min( [$futur_pos_y$ - $y_lower_limit$, 0] )",
      "$booster$" : "$distance_y_reward$ + $acceleration_y_reward$ + $speed_y_reward$ + $height_boundaries$+ $m_fuel$/$m_fuel_ini$"
    },
    "_stop_episode" : "stop episode when goal is reach.", 
    "_stop_episode_info1" : "If feature has 1 value, its feature's value must be equal", 
    "_stop_episode_info2" : "if feature has 2 values [min_limit, max_limit]. Criterion is bounded feature >= min_limit and feature <= max_limit",
    "stop_episode" : {
      "pos_y" : [1, 0],
      "acceleration_y" : [-2,2],
      "speed_y" : [-2,2]
  }
}