{
    "states_variables" : ["pos_y", "speed_y", "weight_rocket"],
    "agent_variables" : ["booster"],
    "initial_values" : {
      "pos_y" : [175.0],
      "speed_y": [0.0],
      "weight_rocket" : [105],
      "booster" : [0.0],
      "angle" : [0.0],
      "alpha" : [0.0],
      "acceleration_y": [0.0],
      "m_fuel" : [100],
      "futur_pos_y" : [175.0],
      "weight_dry_rocket" : [5],
      "G" : [1.62],
      "m_fuel_ini" : [100.0],
      "pos_y_star": [0.0],
      "pos_y_ini" : [175.0]
      },
    "_limit" : ["min", "max", "n_bins"],
    "limit" : {
      "pos_y" : [0.0, 200.0, 51],
      "speed_y": [-50.0, 50.0, 21],
      "weight_rocket" : [0.0, 200.0, 201],
      "angle" : [-0.8, 0.8, 17],
      "booster" : [0.0, 1.0, 3],
      "alpha" : [-0.1, 0.1, 3],
      "acceleration_y": [-20.0, 20.0, 21],
      "m_fuel" : [0.0, 100, 201]
    },
    "n_action" : {
      "booster": {"0" : 0.0, "1" : 0.5, "2" : 1.0}
    },
    "_action_to_take" : "How action change agent_variables, value is taken from n_action dictionary",
    "action_to_take" : {
      "booster": {"$booster$" : "$action$"}
    },
    "_equations" : "compute values for each equation. Variable are updated after the loop",
    "equations_variables": {
        "$F$" : "600",
        "$m_fuel$" : "$m_fuel$ - $booster$ *10 -$angle$ *10",
        "$weight_rocket$" : "$weight_dry_rocket$ + $m_fuel$",
        "dt" : "0.5",
        "$theta$" : "0.0",
        "$y_0$" : "$pos_y$",
        "$Vy_0$" : "$speed_y$",
        "$angle$" : "$theta$ + $alpha$",
        "$acceleration_y$" : "($F$/(5+$weight_rocket$) * np.cos($angle$)) * $booster$ - $G$",
        "$speed_y$" : "($F$/(5+$weight_rocket$) * np.cos($angle$)) * $booster$ * $dt$ - $G$ * $dt$ + $Vy_0$",
        "$pos_y$": "(0.5 * $F$/(5+$weight_rocket$) * np.cos($angle$)) * $booster$ * $dt$**2 - $G$ * $dt$**2 + $Vy_0$ * $dt$ + $y_0$",
        "$futur_pos_y$" : "$pos_y$ + 3 * $speed_y$"
    },
    "equations_rewards": {
      "distance_y_reward" : "np.abs( $pos_y$ - $pos_y_star$)",
      "acceleration_limit" : "5",
      "acceleration_y_reward" : "np.exp(1) - np.exp( np.max([ np.max( np.abs($acceleration_y$)/acceleration_limit), 1 ]) )",
      "speed_limit" : "17",
      "speed_y_reward" : "np.exp(1) - np.exp( np.max([ np.max( np.abs($speed_y$)/speed_limit ), 1 ]) )",
      "y_lower_limit" : "0",
      "y_upper_limit" : "200",
      "upper_boundary": "np.exp(np.min([ np.min(-$futur_pos_y$ + y_upper_limit), 0]))",
      "lower_boundary": "np.exp(np.min([ np.min($futur_pos_y$ -y_lower_limit), 0]))",
      "height_boundaries" : "-2 + lower_boundary + upper_boundary",
      "ratio_fuel" : "$m_fuel$/$m_fuel_ini$",
      "_booster" : "3 * distance_y_reward + acceleration_y_reward + speed_y_reward + 2 * height_boundaries + 0.5* ratio_fuel",
      "$booster$" : "-distance_y_reward - np.abs($speed_y$)" 
    }, 
    "_stop_episode" : "stop episode when goal is reach.", 
    "_stop_episode_info1" : "If feature has 1 value, its feature's value must be equal", 
    "_stop_episode_info2" : "if feature has 2 values [min_limit, max_limit]. Criterion is bounded feature >= min_limit and feature <= max_limit",
    "stop_episode" : {
      "pos_y" : [-10, 10],
      "acceleration_y" : [-2,2]
      },
    "condition_stop_episode_info" : "'all' if all condition must be true, any if one must be true",
    "condition_stop_episode" : "all"
}